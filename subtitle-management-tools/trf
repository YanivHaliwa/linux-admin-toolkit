#!/usr/bin/env python3
import warnings
warnings.filterwarnings("ignore", category=UserWarning)

import sys
import os
from deep_translator import GoogleTranslator, MyMemoryTranslator
import re
import chardet   
from colorama import init, Fore, Style
import time
import signal
import cohere
import json
from collections import deque
from langdetect import detect
from openai import OpenAI
import anthropic
import time
import signal

#version 14.6.25

DEBUG = False

 
global lang_mapping
lang_mapping = {
        'he': {'full': 'hebrew', 'google': 'iw'},  # Hebrew uses 'iw' in Google Translate
        'en': {'full': 'english', 'google': 'en'}, 
        'es': {'full': 'spanish', 'google': 'es'},
        'fr': {'full': 'french', 'google': 'fr'},
        'de': {'full': 'german', 'google': 'de'},
        'it': {'full': 'italian', 'google': 'it'},
        'pt': {'full': 'portuguese', 'google': 'pt'},
        'ru': {'full': 'russian', 'google': 'ru'},
        'zh': {'full': 'chinese', 'google': 'zh'},
        'ja': {'full': 'japanese', 'google': 'ja'},
        'ko': {'full': 'korean', 'google': 'ko'},
        'ar': {'full': 'arabic', 'google': 'ar'}
    }

co = cohere.Client(os.getenv("COHERE_API_KEY"))
modelco= 'command-nightly'

client_oi = OpenAI(api_key=os.environ["OPENAI_API_KEY"])
modelgp = "gpt-4o"


try:
    client_cl = anthropic.Anthropic(api_key=os.environ.get("ANTHROPIC_API_KEY"))
    model_cl = "claude-3-opus-20240229"
except TypeError:
    # For newer versions of the Anthropic SDK that don't accept 'proxies'
    import httpx
    client_cl = anthropic.Anthropic(
        api_key=os.environ.get("ANTHROPIC_API_KEY"),
        http_client=httpx.Client(follow_redirects=True)
    )
    model_cl = "claude-3-opus-20240229"

# Initialize colorama
init()
filevalid=False


def clean_unicode_file(input_path):
    # Regexes for invisible Unicode, control chars, and <0x...>
    # 1. Bidi marks, zero-width, formatting junk
    bidi_junk = r'[\u202A-\u202E\u2066-\u2069\u200B-\u200F\uFEFF]'
    # 2. Any ASCII control chars except \n, \r, \t
    control_junk = r'[\x00-\x08\x0B\x0C\x0E-\x1F]'
    # 3. Literal <0x202a>, <0x0c>, etc (case insensitive)
    hex_artifact = r'<0x[0-9a-fA-F]+>'

    try:
        # First try to read as text with encoding detection
        with open(input_path, 'rb') as file:
            raw_data = file.read()
        
        detected_encoding = chardet.detect(raw_data)['encoding'] or 'utf-8'
        if DEBUG:
            print(f"Detected encoding: {detected_encoding}")
        
        # Decode to string
        try:
            text_content = raw_data.decode(detected_encoding)
        except (UnicodeDecodeError, TypeError):
            try:
                text_content = raw_data.decode('utf-8', errors='ignore')
            except:
                text_content = raw_data.decode('latin-1', errors='ignore')
        
        # Split into lines and clean
        lines = text_content.splitlines()
        cleaned_lines = []
        
        for line in lines:
            # Apply regex cleaning on string data
            line = re.sub(bidi_junk, '', line)
            line = re.sub(control_junk, '', line)
            line = re.sub(hex_artifact, '', line, flags=re.IGNORECASE)
            # Remove trailing spaces/dots from some "dot leader" mess
            line = re.sub(r'[\.Â·]+$', '', line)
            line = line.strip()
            if line:  # Only keep non-empty lines
                cleaned_lines.append(line)
                
        # Join the cleaned lines into a single string
        cleaned_text = '\n'.join(cleaned_lines)
        output_path = input_path + "tempcleaned.txt"
        
        if DEBUG:
            print(f"Writing cleaned content to: {output_path}")
            print(f"Cleaned lines count: {len(cleaned_lines)}")
        
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(cleaned_text)
        
        if DEBUG:
            print(f"File created successfully: {output_path}")
        
        return output_path
        
    except Exception as e:
        if DEBUG:
            print(f"Error in clean_unicode_file: {e}")
        # Fallback: return original file content
        try:
            with open(input_path, 'r', encoding='utf-8', errors='ignore') as file:
                return file.read()
        except:
            with open(input_path, 'r', encoding='latin-1') as file:
                return file.read()

def translate_claude(text_cl, source_lang='english', target_lang='hebrew'):
    try:
        summary = ""
        if text_cl is None:
            return summary
        elif text_cl.strip() == "":
            return summary 
        else:
            # Request translation from Claude API
            with client_cl.messages.stream(
                model=model_cl,
                max_tokens=4000,
                system=f"""
                    you are translator expert in {source_lang} and {target_lang}.
                    you going to translate from {source_lang} to {target_lang}.
                    your respond should be only the translate nothing else.
                    i need you to think on the context from user not just cold translate. 
                    i also need you figure slang of people . dont just transalte words but think how they use it and that translte. 
                    must remember you translating srt files so i need you to be precise with timestamp and title numbers as in original exactly.
                    CRITICAL: DO NOT modify timecode format. Keep exact format: "00:01:04,565 --> 00:01:06,033" (no spaces around colons, use double arrow -->)
                    also remember you cant exaplain anything in process you only trnaslating accoding to source only.
                    do not use in your answer any qoute or other words that not in the original you stick to translate only
                    do not add words "plaintext" in your translation just if its exist in the original  
                    do not write anything but {target_lang} translate ONLY        
                """,
                messages=[
                    {
                        "role": "user", 
                        "content": text_cl
                    },
                ],
                temperature=1
            )  as stream:
                    for text in stream.text_stream:
                          summary+=text      
            return summary
    except Exception as e:
        if DEBUG:
            print(f"Claude translation error: {e}")
        pass

def translate_gpt(texttogpt, source_lang='english', target_lang='hebrew'):
    try:
        text=""
        if texttogpt is None:
            return text
        elif texttogpt.strip()=="":
            return text 
        else:
            response = client_oi.chat.completions.create(
            model=modelgp,
            messages=[
                {
                    'role': 'system',
                    'content': f"""
                    you are translator expert in {source_lang} and {target_lang}. 
                    you going to translate from {source_lang} to {target_lang}.
                    your respond should be only the translate nothing else.
                    i need you to think on the context from user not just cold translate. 
                    i also need you figure slang of people . dont just transalte words but think how they use it and that translte. 
                    must remember you translating srt files so i need you to be precise with timestamp and title numbers as in original exactly.
                    CRITICAL: DO NOT modify timecode format. Keep exact format: "00:01:04,565 --> 00:01:06,033" (no spaces around colons, use double arrow -->)
                    also remember you cant exaplain anything in process you only trnaslating accoding to source only.
                    do not use in your answer any qoute or other words that not in the original you stick to translate only
                    do not add words "plaintext" in your translation just if its exist in the original  
                    do not write anything but {target_lang} translate ONLY     
                """
                },
                
                {
                    "role": "user", 
                    "content":f"translate this from {source_lang} to {target_lang}: "+texttogpt
                },
                
            ],
            stream=True,
            temperature=1,
            max_tokens=3000
            )
        
            for chunk in response:
                if chunk.choices[0].delta.content is not None:  
                    temptext = chunk.choices[0].delta.content
                    text+=temptext
            return text
    except Exception as e:
        if DEBUG:
            print(f"GPT translation error: {e}")
        pass
def translate_cohere(texttoco, source_lang='english', target_lang='hebrew'):   
    try:
        text=""
        token_count=0
        for event in co.chat_stream(
                model=modelco,
                message=f"translate this from {source_lang} to {target_lang}: "+texttoco,
                temperature=0.7,
                preamble=f"""you are transaltor expert in {source_lang} and {target_lang}. 
                you going to translate from {source_lang} to {target_lang}.
                your respond should be only the translate nothing else.
                i need you to think on the context from user not just cold translate. 
                i also need you figure slang of people . dont just transalte words but think how they use it and that translte. 
                must remember you translating srt files so i need you to be precise with timestamp and title numbers as in original exactly.
                CRITICAL: DO NOT modify timecode format. Keep exact format: "00:01:04,565 --> 00:01:06,033" (no spaces around colons, use double arrow -->)
                also remember you cant exaplain anything in process you only trnaslating accoding to source only.
                do not use in your answer any qoute or other words that not in the original you stick to translate only 
                do not add words "plaintext" in your translation just if its exist in the original 
                do not write anything but {target_lang} translate ONLY 
                """
            ):

            if event.event_type == "text-generation":
                response_tokens = len(event.text.split())  # Estimate token count
                token_count += response_tokens
                text+=event.text
            elif event.event_type == "stream-end":
                break
        return text
    
    except Exception as e:
        if DEBUG:
            print(f"Cohere translation error: {e}")
        pass
 

def read_file_with_detected_encoding(file_path):
    with open(file_path, 'rb') as file:
        raw_data = file.read()
    
    detected_encoding = chardet.detect(raw_data)['encoding']
    
    try:
        # Try to read the file with the detected encoding
        with open(file_path, 'r', encoding=detected_encoding) as file:
            return file.read()
    except (UnicodeDecodeError, TypeError):
        # Fallback to 'utf-8' encoding if detected encoding fails
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read()
        except UnicodeDecodeError:
            # Fallback to 'latin-1' if 'utf-8' fails
            with open(file_path, 'r', encoding='latin-1') as file:
                return file.read()
 

def is_timecode_line(line):
    return re.match(r'\d{2}:\d{2}:\d{2},\d{3} --> \d{2}:\d{2}:\d{2},\d{3}', line)


def fix_timecode_format(line):
    """Fix timecode format that may have been modified by AI translators"""
    # Fix spaces around colons in timecode: "00: 01: 04,565" -> "00:01:04,565"
    line = re.sub(r'(\d{2})\s*:\s*(\d{2})\s*:\s*(\d{2}),(\d{3})', r'\1:\2:\3,\4', line)
    
    # Fix arrow format: single arrow "->" to double arrow "-->"
    # Also handle cases where spaces might be around the arrow
    line = re.sub(r'(\d{2}:\d{2}:\d{2},\d{3})\s*->\s*(\d{2}:\d{2}:\d{2},\d{3})', r'\1 --> \2', line)
    
    return line

def format_translated_chunks(translated_chunks):
    formatted_blocks = []
    current_block = []
   # rlm = "\u200F"  # Right-to-Left Mark

    expecting_number = True

    for chunk in translated_chunks:
        lines = chunk.split('\n')
        for line in lines:
            line = line.strip()
            if not line:
                continue

            # Fix timecode format if this looks like a timecode line
            if re.search(r'\d{1,2}\s*:\s*\d{2}\s*:\s*\d{2},\d{3}', line):
                line = fix_timecode_format(line)

            processed_line = line if line else line

            if expecting_number:
                if line.isdigit():
                    if current_block:  # Finalize the current block if not empty
                        formatted_blocks.append('\n'.join(current_block) + '\n')
                        current_block = []
                    current_block.append(processed_line)
                    expecting_number = False
                else:
                    current_block.append(processed_line)
            else:
                current_block.append(processed_line)
                if is_timecode_line(line):
                    expecting_number = True

    if current_block:  # Add the last block if exists
        formatted_blocks.append('\n'.join(current_block))

    return '\n'.join(formatted_blocks)


def smart_chunker(text, chunk_size):
    chunks = []
    start_index = 0
    while start_index < len(text):
        # Find the end index for this chunk, which is start index + chunk size
        end_index = start_index + chunk_size
        
        # If the end index is beyond the length of the text, set it to the length of the text
        if end_index > len(text):
            end_index = len(text)
        else:
            # Ensure the end index does not break in the middle of a subtitle entry
            # Find the nearest previous newline character to avoid cutting a subtitle in half
            end_index = text.rfind('\n\n', start_index, end_index) + 2
        
        # Append the current chunk to the list of chunks
        chunks.append(text[start_index:end_index])
        
        # Update the start index for the next chunk
        start_index = end_index

    return chunks


def text_chunker(text, chunk_size):
    chunks = []
    start_index = 0
    if DEBUG:
        print(f"{len(text)} characters in text, chunking into {chunk_size} character chunks")
        
    while start_index < len(text):
        if DEBUG:
            print(f"Processing chunk starting at index {start_index}")
            
        # Find the end index for this chunk, which is start index + chunk size
        end_index = start_index + chunk_size
        
        # If the end index is beyond the length of the text, set it to the length of the text
        if end_index > len(text):
            end_index = len(text)
        else:
            # Ensure the end index does not break in the middle of a paragraph
            # Find the nearest previous newline character to avoid cutting a paragraph in half
            newline_pos = text.rfind('\n\n', start_index, end_index)
            if newline_pos != -1:  # Found a double newline
                end_index = newline_pos + 2
            else:
                # No double newline found, try single newline
                newline_pos = text.rfind('\n', start_index, end_index)
                if newline_pos != -1:
                    end_index = newline_pos + 1
                # If no newline found at all, keep the original end_index
        
        # Ensure we don't get stuck in an infinite loop
        if end_index <= start_index:
            end_index = start_index + 1  # Move at least one character forward
            
        # Append the current chunk to the list of chunks
        chunk = text[start_index:end_index]
        chunks.append(chunk)
        
        if DEBUG:
            print(f"Created chunk {len(chunks)}: {len(chunk)} characters")
        
        # Update the start index for the next chunk
        start_index = end_index
        
    if DEBUG:
        print(f"Total chunks created: {len(chunks)}")
        for i, chunk in enumerate(chunks):
            print(f"Chunk {i+1}: {len(chunk)} characters")
    
    return chunks

def translate_file(file_path, translator, target_lang='he', target_lang_full='hebrew'):
    # Get Google language code from global mapping
    global lang_mapping
    lang_info = lang_mapping.get(target_lang, None)
    google_lang_code = lang_info['google'] if lang_info else target_lang
    
    if DEBUG:
        print(f"Translator: {translator}, Target language: {target_lang_full}")
    print(f"Translator: {translator}")
    
    file_dir, file_name = os.path.split(file_path)
    file_name_without_ext, file_ext = os.path.splitext(file_name)
    
    # # Skip files that already include target language in filename
    # lang_suffix = target_lang if target_lang != 'he' else 'heb'
    # if lang_suffix in file_name:
    #     return  
    lang_suffix = target_lang   
    output_file_path = os.path.join(file_dir, f"{file_name_without_ext}-{lang_suffix}{file_ext}")
    text = ""
    newtext = ""
    file_extension = file_ext
   
    try:
        
        # Only clean file if not .srt (to preserve subtitle formatting)
        if not file_path.lower().endswith('.srt'):
            clean_file = clean_unicode_file(file_path)
        else:
            clean_file = file_path
        text = read_file_with_detected_encoding(clean_file)
        # Exit after cleaning to avoid further processing
        
        # Check file size and warn if too large
        file_size_mb = len(text.encode('utf-8')) / (1024 * 1024)
        if file_size_mb > 5:  # If larger than 5MB
            print(f"Warning: Large file detected ({file_size_mb:.1f}MB). This may take a while...")
            proceed = input("Continue? (y/n): ")
            if proceed.lower() != 'y':
                print("Translation cancelled.")
                return
        
        s = detect(text)
        full_lang_detected = get_lang_full_name(s)
        if full_lang_detected is None:
            full_lang_detected = s
        print(f"Detected language: {full_lang_detected}")
        print(f"Target language: {target_lang_full}\n")
        
        translated_chunks = []
        
        if file_extension == '.srt':
            chunk_size = 3500
            text_chunks = smart_chunker(text, chunk_size)
        else:
            # For large text files, use very small chunks to prevent memory issues
            chunk_size = 3500  # Much smaller chunks for stability
            text_chunks = text_chunker(text, chunk_size)
            # text_chunks = smart_chunker(text, chunk_size)
            
        translated_chunks = []
        
        if DEBUG:
            print(f"Total text length: {len(text)} characters")
            print(f"Chunk size: {chunk_size} characters")
            print(f"Number of chunks: {len(text_chunks)}")
           
        translated_chunks = []
        
        for i, chunk in enumerate(text_chunks):
            print(f"Translating chunk {i+1}/{len(text_chunks)} ({round((i+1)/len(text_chunks)*100, 1)}%)", end='\r')
            
            try:
                source_language = detect(chunk)
                
                # Map detected language to Google Translate format
                source_lang_code = 'iw' if source_language == 'he' else source_language
                
                # Determine source language name for AI translators
                source_lang_name = 'hebrew' if source_language == 'he' else 'english'
                
                if translator=="google":
                    # Create GoogleTranslator with proper source and target languages
                    trs = GoogleTranslator(source=source_lang_code, target=google_lang_code)
                    translated_chunk = trs.translate(chunk)
                elif translator=="cohere":
                    translated_chunk = translate_cohere(chunk, source_lang_name, target_lang_full)
                elif translator=="chatgpt":
                    translated_chunk = translate_gpt(chunk, source_lang_name, target_lang_full)
                elif translator=="claude":
                    translated_chunk = translate_claude(chunk, source_lang_name, target_lang_full)
                     
                translated_chunks.append(translated_chunk)
                
                # Add small delay to prevent rate limiting
                time.sleep(0.05)
                
            except Exception as e:
                print(f"\nError with chunk {i+1}: {e}")
                translated_chunks.append(chunk)  # Keep original if translation fails
            
        
        #after translating all chunks, format them
        formatted_translated_text = format_translated_chunks(translated_chunks)

        # Write the formatted translated text to the output file
        with open(output_file_path, 'w') as output_file:
            output_file.write(formatted_translated_text)
        
        success= f"{Fore.YELLOW}{Style.BRIGHT}Translation successful. Translated file saved to: {Style.RESET_ALL}"
        print(f"{success}'{output_file_path}'.")
        
        if not file_path.lower().endswith('.srt'):
            try:
                if os.path.exists(clean_file):
                    os.remove(clean_file)
                    if DEBUG:
                        print(f"Deleted temporary cleaned file: {clean_file}")
            except Exception as e:
                if DEBUG:
                    print(f"Error deleting temporary cleaned file: {e}")


    except Exception as e:
        print(f"Translation failed for {file_path}. Error:", str(e))
    except KeyboardInterrupt:
        print("\r" + " " * 100 + "\r" + "exiting..", flush=True)
        sys.exit(0)

def collect_srt_files(directory, target_lang='he'):
    srt_files = []
    lang_suffix = target_lang if target_lang != 'he' else 'heb'
    
    # Loop through only files in the specified directory
    for file in os.listdir(directory):
        file_path = os.path.join(directory, file)
        # Check if it's a file and meets the criteria
        if os.path.isfile(file_path) and (file.endswith(".srt") or file.endswith(".txt") or file.endswith(".md")) and (lang_suffix not in file and f".{target_lang}." not in file):
            srt_files.append(file_path)
    return srt_files

def is_file_empty(file_path):
    try:
        with open(file_path, 'r', encoding='utf-8') as file:
            content = file.read().strip()
            if len(content) == 0:
                return True
            else:
                return False
    except Exception as e:
        return False

def get_lang_full_name(lang):
    global lang_mapping
    lang_info = lang_mapping.get(lang, None)
    return lang_info['full'] if lang_info else None

def main():
    global lang_mapping
    translator = "google"
    target_lang = "he"  # Default to Hebrew
    target_lang_full = get_lang_full_name(target_lang)
    
    if len(sys.argv) < 3 or '-h' in sys.argv or '--help' in sys.argv:
        print("""Usage: python trf2.py -f <file_path> OR -d <directory_path> [options]

Description:
    Translate text files with special optimization for subtitle files
    Supports: .srt (subtitles), .txt (text files), .md (markdown files)
    Default translation is to Hebrew using Google Translate
    
    Note: Subtitle files (.srt) get special formatting preservation for timestamps

Required Arguments:
    -f <file_path>      Translate a single file (.srt, .txt, .md)
    -d <directory_path> Translate all supported files in directory

Optional Arguments:
    -l <lang_code>      Target language (default: he for Hebrew)
                        Supported codes: he, en, es, fr, de, it, pt, ru, zh, ja, ko, ar
                        Languages: Hebrew, English, Spanish, French, German, Italian,
                                  Portuguese, Russian, Chinese, Japanese, Korean, Arabic

    Translator options (default is Google Translate):
        -gp             Use ChatGPT for translation
        -co             Use Cohere AI for translation  
        -cl             Use Claude AI for translation

Examples:
    python trf2.py -f subtitle.srt                    # Translate subtitle to Hebrew (default)
    python trf2.py -f document.txt -l en              # Translate text file to English
    python trf2.py -f readme.md -l es -gp             # Translate markdown to Spanish with ChatGPT
    python trf2.py -d /path/files -l fr -cl           # Translate all files to French with Claude
                """)
        sys.exit(1)

    # Parse arguments
    args = sys.argv[1:]
    option = None
    path = None
    
    i = 0
    while i < len(args):
        if args[i] == '-f' or args[i] == '-d':
            option = args[i]
            if i + 1 < len(args):
                path = args[i + 1]
                i += 2
            else:
                print("Error: Missing path after option")
                sys.exit(1)
        elif args[i] == '-l':
            if i + 1 < len(args):
                target_lang = args[i + 1]
                target_lang_full = get_lang_full_name(target_lang)
                if not target_lang_full:
                    print(f"Error: Unsupported language code '{target_lang}'")
                    print("Supported languages: ", ', '.join([f"{k} ({v['full']})" for k, v in lang_mapping.items()]))
                    sys.exit(1)
                i += 2
            else:
                print("Error: Missing language code after -l")
                sys.exit(1)
        elif args[i] == '-co':
            translator = "cohere"
            i += 1
        elif args[i] == '-gp':
            translator = "chatgpt"
            i += 1
        elif args[i] == '-cl':
            translator = "claude"
            i += 1
        else:
            i += 1

    if not option or not path:
        print("Error: Missing required -f or -d option with path")
        sys.exit(1)

    if DEBUG:
        print(f"Debug: option={option}, path={path}, translator={translator}, target_lang={target_lang}")

    files_srt = []
    filevalid = False

    if option == '-f':
        files_srt.append(path)
        lang_suffix = target_lang if target_lang != 'he' else 'heb'
        if (path.endswith(".srt") or path.endswith(".txt") or path.endswith(".md")) and (lang_suffix not in path and f".{target_lang}." not in path):
            filevalid = True

    elif option == '-d':
        files_srt = collect_srt_files(path, target_lang)
        if files_srt:
            filevalid = True
        
    else:
        invalidtemp = f"{Fore.RED}{Style.BRIGHT}Invalid option. Use -f for file or -d for directory.{Style.RESET_ALL}"
        print(invalidtemp)
        sys.exit(0)
    
    try:
        if filevalid and files_srt:
            filetemp = f"{Fore.BLUE}{Style.BRIGHT}files to translate:{Style.RESET_ALL}"
            print(filetemp)
            for f in files_srt:
                print(f)
            print()
     
            for file_path in files_srt:
                empty = is_file_empty(file_path)
                if empty:
                    print(f"{Fore.YELLOW}{Style.BRIGHT}File is empty. Skipping:{Style.RESET_ALL}", file_path)
                else:
                    trtemp = f"{Fore.GREEN}{Style.BRIGHT}Translating:{Style.RESET_ALL}"
                    print(trtemp, file_path)
                    translate_file(file_path, translator, target_lang, target_lang_full)

                    intemp = f"{Fore.GREEN}{Style.BRIGHT}\nwould you like to delete the old source files? y/n \n{Style.RESET_ALL}"
                    delconfirm = input(intemp)
                    if delconfirm.lower() == "y":
                        for fi in files_srt:
                            if os.path.exists(fi):
                                os.remove(fi)  # Delete the file
                        print(f"\nDeleted")
        else:     
            nothing = f"{Fore.RED}{Style.BRIGHT}Nothing to translate{Style.RESET_ALL}"
            print(nothing)
    except KeyboardInterrupt:
        print("\r" + " " * 100 + "\r" + "exiting..", flush=True)
        sys.exit(0)
                 
if __name__ == "__main__":
    main()


